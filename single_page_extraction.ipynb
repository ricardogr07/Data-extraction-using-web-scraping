{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install necessary libraries if they are not present\n",
                "!pip install requests\n",
                "!pip install beautifulsoup4\n",
                "!pip install pandas"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "Fl7zUUWBV6mS"
            },
            "outputs": [],
            "source": [
                "# Import relevant packages\n",
                "import requests\n",
                "from bs4 import BeautifulSoup\n",
                "import pandas as pd\n",
                "import datetime\n",
                "import random"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "HFn7QeLu3xOO"
            },
            "source": [
                "# Data Extraction through Web Scraping.\n",
                "\n",
                "## Introduction.\n",
                "\n",
                "Almost 10 years ago, the job of a data scientist was labeled by Harvard Business Review as \"the sexiest job of the 21st century\" [(Davenport & Patil 2012)](https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century). Since then, there has been a steady increase in the demand for data experts, and it is expected that both job creation and salaries will continue to rise in the coming years. The following articles illustrate this situation:\n",
                "\n",
                "https://www.smithhanley.com/2022/01/04/data-science-in-2022/\n",
                "https://www.bbva.com/en/big-data-the-demand-for-expert-talent-continues-to-grow/\n",
                "\n",
                "The cited studies refer to labor markets in Europe and the United States. Suppose you are in charge of developing a study of the labor market for data scientists in Latin America, for which you need to build a database with job offers published in different countries of the region.\n",
                "\n",
                "The objective of this task is to use web scraping techniques to extract data on job offers for data scientists published on an open job portal (www.linkedin.com/jobs).\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "M_ICRQTPxUYI"
            },
            "source": [
                "#### 1. Go to the website www.linkedin.com/jobs, click on the `Search Jobs` button, and search for jobs for *data scientist* in your country's capital (or another city of interest). Inspect and analyze the source code of the results page to understand the structure of its HTML code."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "9PBdfKE4xUYJ"
            },
            "source": [
                "#### 2. Extract the list of job postings returned by your search on LinkedIn."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "knsl-A5cxUYK"
            },
            "outputs": [],
            "source": [
                "# Define the position and location to scrape\n",
                "position = 'data scientist'\n",
                "url_friendly_position = position.replace(\" \",\"%20\")\n",
                "location = 'Monterrey'\n",
                "url_search = 'https://www.linkedin.com/jobs/search/?keywords=%s&location=%s'%(url_friendly_position, location)\n",
                "print(url_search)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "6jJqLmYP3Ux5"
            },
            "outputs": [],
            "source": [
                "# To prevent the website from thinking you are a bot, use one of the following headers when making the request:\n",
                "\n",
                "# List of headers\n",
                "headers = [\n",
                "    {'User-Agent': 'Mozilla/5.0'},\n",
                "    {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.84 Safari/537.36'},\n",
                "    {'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Mobile Safari/537.36'},\n",
                "    {'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Mobile Safari/537.36'},\n",
                "    {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.84 Safari/537.36'}\n",
                "]\n",
                "\n",
                "# Randomly select one header\n",
                "head = random.choice(headers)\n",
                "\n",
                "# Print the selected header\n",
                "print(f\"Using header: {head}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "BqChn4MZxUYL"
            },
            "outputs": [],
            "source": [
                "# Obtain a list of jobs related to the position and location\n",
                "response = requests.get(url_search, headers=head)\n",
                "print(response)\n",
                "soup = BeautifulSoup(response.text, 'html.parser')\n",
                "joblist = soup.find('ul', class_=\"jobs-search__results-list\")\n",
                "alljobs = joblist.find_all('li')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "W5AERO-kxUYM"
            },
            "source": [
                "#### 3. Select only the first job posting from the list and extract the following information: job title, company name, location, and job URL.\n",
                "\n",
                "Note: By location, we mean the city, district, or municipality specified in the posting."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(alljobs[0])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "info = alljobs[0].find('div', class_=\"base-search-card__info\")\n",
                "title = info.find('h3', class_=\"base-search-card__title\").text.strip()\n",
                "company = info.find('h4', class_=\"base-search-card__subtitle\").text.strip()\n",
                "metadata = alljobs[0].find('div', class_=\"base-search-card__metadata\")\n",
                "location_element = metadata.find('span', class_=\"job-search-card__location\")\n",
                "location_job = location_element.text.strip()\n",
                "\n",
                "joburl = alljobs[0].find('a', class_=\"base-card__full-link\")['href']\n",
                "\n",
                "# Information about the first job\n",
                "print(f'Título: {title}')\n",
                "print(f'Empresa: {company}')\n",
                "print(f'Ubicación: {location_job}')\n",
                "print(f'URL del trabajo: {joburl}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "pOcOSwFuxUYN"
            },
            "outputs": [],
            "source": [
                "# Create a list of dictionaries with job information\n",
                "\n",
                "jobs = []\n",
                "\n",
                "for job in alljobs:\n",
                "    info = job.find('div', class_=\"base-search-card__info\")\n",
                "    title = info.find('h3', class_=\"base-search-card__title\").text.strip()\n",
                "    company = info.find('h4', class_=\"base-search-card__subtitle\").text.strip()\n",
                "    \n",
                "    metadata = job.find('div', class_=\"base-search-card__metadata\")\n",
                "    location_element = metadata.find('span', class_=\"job-search-card__location\")\n",
                "    location_job = location_element.text.strip()\n",
                "    \n",
                "    joburl = job.find('a', class_=\"base-card__full-link\")['href']\n",
                "    \n",
                "    job_info = {\n",
                "        'Location': location_job,\n",
                "        'Title': title,\n",
                "        'Company': company,\n",
                "        'Url': joburl\n",
                "    }\n",
                "\n",
                "    jobs.append(job_info)\n",
                "\n",
                "# Select the first job from the list and extract the relevant information\n",
                "first_job = jobs[0]\n",
                "location_job = first_job['Location']\n",
                "title_job = first_job['Title']\n",
                "company_job = first_job['Company']\n",
                "joburl_job = first_job['Url']\n",
                "\n",
                "# Print the information of the first job\n",
                "print(f'Título: {title_job}')\n",
                "print(f'Empresa: {company_job}')\n",
                "print(f'Ubicación: {location_job}')\n",
                "print(f'URL del trabajo: {joburl_job}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Uf1pPnWTxUYO"
            },
            "source": [
                "#### 4. Based on the previous points, write a routine to extract the information for location, job title, company name, and job URL for all the job postings returned by your LinkedIn search, and store the data in a pandas dataframe.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "yajotXq1xUYO"
            },
            "outputs": [],
            "source": [
                "df_jobs = pd.DataFrame(jobs, columns=['Location', 'Title', 'Company', 'Url'])\n",
                "print(df_jobs.head(1))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "5AzJvKlxxUYP"
            },
            "source": [
                "#### 5. Export your dataframe to a .csv file."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "dL4-uR9_QQOL"
            },
            "outputs": [],
            "source": [
                "# Exportar el DataFrame a un archivo CSV\n",
                "date = datetime.datetime.now().strftime('%Y-%m-%d')\n",
                "position = position.replace(\" \", \"_\")\n",
                "nombre_archivo = f'LinkedIn_{position}_{location}_{date}.csv'\n",
                "df_jobs.to_csv(nombre_archivo, index=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "qi4sOfg6xUYP"
            },
            "source": [
                "#### 6. How many job postings does your dataframe contain, and how many results are there in total from the LinkedIn search? Comment on the differences or matches, and explain what you would need to do to extract all available results from LinkedIn (in words, implementation is not necessary)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "Tg8fwWroxUYP"
            },
            "outputs": [],
            "source": [
                "ofertas_df = df_jobs['Url'].count()\n",
                "ofertas_linkedin = int(soup.find('span', {'class': 'results-context-header__job-count'}).text)\n",
                "print(f'The number of job postings in the DataFrame is {ofertas_df}, while the total number of results on LinkedIn is {ofertas_linkedin}.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "If there is a discrepancy between these two results, it is likely because I have only extracted the job postings from the first page of LinkedIn results. LinkedIn displays a limited number of postings per page, and if there are more postings, they are shown on subsequent pages.\n",
                "\n",
                "To extract all available results from LinkedIn, you would need to implement a loop that iterates through all the result pages. This would involve extracting the URL for the next page from the current page, then performing web scraping on that page and adding the information to the dataframe, and so on, until there are no more pages or results.\n",
                "\n",
                "However, this process can be complex and require careful handling to avoid being blocked by LinkedIn due to generating too many requests in a short period of time. It is important to mention that web scraping LinkedIn is against their terms of service, so this would likely be considered a bad practice.\n"
            ]
        }
    ],
    "metadata": {
        "colab": {
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
