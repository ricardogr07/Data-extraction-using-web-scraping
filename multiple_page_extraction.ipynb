{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (17.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyarrow) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries if they are not present\n",
    "!pip install requests\n",
    "!pip install beautifulsoup4\n",
    "!pip install pyarrow\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_posted_dict = {\n",
    "    'ALL': '',\n",
    "    'MONTH': 'r2592000',\n",
    "    'WEEK': 'r604800',\n",
    "    'DAY': 'r86400'\n",
    "}\n",
    "remote_dict = {\n",
    "    'ALL': '',\n",
    "    'ON-SITE': '1',\n",
    "    'REMOTE': '2',\n",
    "    'HYBRID': '3'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_user_agent():\n",
    "\n",
    "    headers = [\n",
    "        {'User-Agent': 'Mozilla/5.0'},\n",
    "        {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.84 Safari/537.36'},\n",
    "        {'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Mobile Safari/537.36'},\n",
    "        {'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Mobile Safari/537.36'},\n",
    "        {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.84 Safari/537.36'}\n",
    "    ]\n",
    "\n",
    "    selected_header = random.choice(headers)\n",
    "    return selected_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_main_linkedin_url(position, location, distance=10, time_posted='ALL', remote='ALL'):\n",
    "   \n",
    "    # Base URL for LinkedIn job search\n",
    "    base_url = 'https://www.linkedin.com/jobs/search/'\n",
    "    \n",
    "    # Replace spaces in position with URL encoding\n",
    "    url_friendly_position = position.replace(\" \", \"%20\")\n",
    "    \n",
    "    # Construct the query parameters\n",
    "    query_params = f'?keywords={url_friendly_position}&location={location}'\n",
    "    \n",
    "    if distance:\n",
    "        query_params += f'&distance={distance}'\n",
    "    if time_posted:\n",
    "        time_posted_value = time_posted_dict.get(time_posted, '')\n",
    "        query_params += f'&f_TPR={time_posted_value}'\n",
    "    if remote:\n",
    "        remote_value = remote_dict.get(remote, '')\n",
    "        query_params += f'&f_WT={remote_value}'\n",
    "    \n",
    "    # Combine base URL with query parameters\n",
    "    url_search = base_url + query_params\n",
    "    \n",
    "    return url_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_next_10_positions(position, location,start_position, distance=10, time_posted='ALL', remote='ALL'):\n",
    "   \n",
    "    # Base URL for LinkedIn job search\n",
    "    base_url = 'https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search'\n",
    "    \n",
    "    # Replace spaces in position with URL encoding\n",
    "    url_friendly_position = position.replace(\" \", \"%20\")\n",
    "    \n",
    "    # Construct the query parameters\n",
    "    query_params = f'?keywords={url_friendly_position}&location={location}'\n",
    "    \n",
    "    if distance:\n",
    "        query_params += f'&distance={distance}'\n",
    "    if time_posted:\n",
    "        time_posted_value = time_posted_dict.get(time_posted, '')\n",
    "        query_params += f'&f_TPR={time_posted_value}'\n",
    "    if remote:\n",
    "        remote_value = remote_dict.get(remote, '')\n",
    "        query_params += f'&f_WT={remote_value}'\n",
    "    query_params += f'&position=1&pageNum=0&start={start_position}'\n",
    "    \n",
    "    # Combine base URL with query parameters\n",
    "    url_search = base_url + query_params\n",
    "    \n",
    "    return url_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using header: {'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Mobile Safari/537.36'}\n"
     ]
    }
   ],
   "source": [
    "position = 'Data Scientist'\n",
    "location = 'Monterrey'\n",
    "time_posted = 'ALL'\n",
    "remote = 'ALL'\n",
    "\n",
    "header = get_random_user_agent()\n",
    "\n",
    "main_url = generate_main_linkedin_url(position, location,time_posted=time_posted, remote=remote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_jobs_until_success(url, headers):\n",
    "    got_200 = False\n",
    "    while not got_200:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        got_200 = response.status_code == 200\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 533 jobs that will be scraped based on the given conditions.\n"
     ]
    }
   ],
   "source": [
    "response = fetch_jobs_until_success(main_url, header)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "all_jobs = int(soup.find('span', {'class': 'results-context-header__job-count'}).text)\n",
    "print(f'There are a total of {all_jobs} jobs that will be scraped based on the given conditions.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing data for page: 1/54\n",
      "Parsing data for page: 2/54\n",
      "Parsing data for page: 3/54\n",
      "Parsing data for page: 4/54\n",
      "Parsing data for page: 5/54\n",
      "Parsing data for page: 6/54\n",
      "Parsing data for page: 7/54\n",
      "Parsing data for page: 8/54\n",
      "Parsing data for page: 9/54\n",
      "Parsing data for page: 10/54\n",
      "Parsing data for page: 11/54\n",
      "Parsing data for page: 12/54\n",
      "Parsing data for page: 13/54\n",
      "Parsing data for page: 14/54\n",
      "Parsing data for page: 15/54\n",
      "Parsing data for page: 16/54\n",
      "Parsing data for page: 17/54\n",
      "Parsing data for page: 18/54\n",
      "Parsing data for page: 19/54\n",
      "Parsing data for page: 20/54\n",
      "Parsing data for page: 21/54\n",
      "Parsing data for page: 22/54\n",
      "Parsing data for page: 23/54\n",
      "Parsing data for page: 24/54\n",
      "Parsing data for page: 25/54\n",
      "Parsing data for page: 26/54\n",
      "Parsing data for page: 27/54\n",
      "Parsing data for page: 28/54\n",
      "Parsing data for page: 29/54\n",
      "Parsing data for page: 30/54\n",
      "Parsing data for page: 31/54\n",
      "Parsing data for page: 32/54\n",
      "Parsing data for page: 33/54\n",
      "Parsing data for page: 34/54\n",
      "Parsing data for page: 35/54\n",
      "Parsing data for page: 36/54\n",
      "Parsing data for page: 37/54\n",
      "Parsing data for page: 38/54\n",
      "Parsing data for page: 39/54\n",
      "Parsing data for page: 40/54\n",
      "Parsing data for page: 41/54\n",
      "Parsing data for page: 42/54\n",
      "Parsing data for page: 43/54\n",
      "Parsing data for page: 44/54\n",
      "Parsing data for page: 45/54\n",
      "Parsing data for page: 46/54\n",
      "Parsing data for page: 47/54\n",
      "Parsing data for page: 48/54\n",
      "Parsing data for page: 49/54\n",
      "Parsing data for page: 50/54\n",
      "Parsing data for page: 51/54\n",
      "Parsing data for page: 52/54\n",
      "Parsing data for page: 53/54\n",
      "Parsing data for page: 54/54\n"
     ]
    }
   ],
   "source": [
    "jobs = []\n",
    "total_pages = math.ceil(all_jobs/10)\n",
    "for i in range(0,all_jobs, 10):\n",
    "    \n",
    "    current_page = i/10+1\n",
    "\n",
    "    target_url = get_url_next_10_positions(position, location,i,time_posted=time_posted,remote=remote)\n",
    "\n",
    "    response = fetch_jobs_until_success(target_url, header)\n",
    "    \n",
    "    print(f\"Parsing data for page: {int(current_page)}/{total_pages}\")\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    alljobs = soup.find_all('li')\n",
    "\n",
    "    for job in alljobs:\n",
    "        try:\n",
    "            info = job.find('div', class_=\"base-search-card__info\")\n",
    "            title = info.find('h3', class_=\"base-search-card__title\").text.strip() if info else 'N/A'\n",
    "            company = info.find('h4', class_=\"base-search-card__subtitle\").text.strip() if info else 'N/A'\n",
    "\n",
    "            metadata = job.find('div', class_=\"base-search-card__metadata\")\n",
    "            location_element = metadata.find('span', class_=\"job-search-card__location\") if metadata else None\n",
    "            location_job = location_element.text.strip() if location_element else 'N/A'\n",
    "\n",
    "            joburl_element = job.find('a', class_=\"base-card__full-link\")\n",
    "            joburl = joburl_element['href'] if joburl_element else 'N/A'\n",
    "\n",
    "            job_info = {\n",
    "                'Location': location_job,\n",
    "                'Title': title,\n",
    "                'Company': company,\n",
    "                'Url': joburl\n",
    "            }\n",
    "\n",
    "            jobs.append(job_info)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing job: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = pd.DataFrame(jobs, columns=['Location', 'Title', 'Company', 'Url'])\n",
    "df_jobs.replace(\"N/A\", pd.NA, inplace=True)\n",
    "df_jobs = df_jobs.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Monterrey' 'San Pedro Garza García' 'San Nicolás de Los Garza'\n",
      " 'Centro de San Pedro Garza García' 'Garza García' 'Polanco'\n",
      " 'Santa Catarina' 'San Nicolás de los Garza' 'Guadalupe' 'Villa de García'\n",
      " 'Monterrey Metropolitan Area']\n"
     ]
    }
   ],
   "source": [
    "df_jobs['Location'] = df_jobs['Location'].apply(lambda x: x.split(',')[0])\n",
    "unique_locations = df_jobs['Location'].unique()\n",
    "print(unique_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mapping for the location names\n",
    "location_mapping = {\n",
    "    'San Pedro Garza García': 'San Pedro',\n",
    "    'Centro de San Pedro Garza García': 'San Pedro',\n",
    "    'Monterrey Metropolitan Area': 'Monterrey',\n",
    "    'San Nicolás de los Garza': 'San Nicolás',\n",
    "    'San Nicolás de Los Garza': 'San Nicolás',\n",
    "    'Garza García': 'García',\n",
    "    'Santa Catarina': 'Santa Catarina',\n",
    "    'Guadalupe': 'Guadalupe',\n",
    "    'Villa de García': 'García',\n",
    "    'Polanco': 'Polanco'\n",
    "}\n",
    "# Function to rename locations based on the mapping\n",
    "def rename_location(location):\n",
    "    return location_mapping.get(location, location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Monterrey' 'San Pedro' 'San Nicolás' 'García' 'Polanco' 'Santa Catarina'\n",
      " 'Guadalupe']\n"
     ]
    }
   ],
   "source": [
    "# Apply the function to the DataFrame\n",
    "df_jobs['Location'] = df_jobs['Location'].apply(rename_location)\n",
    "\n",
    "# Verify the result\n",
    "unique_locations = df_jobs['Location'].unique()\n",
    "print(unique_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_location_list = ['Monterrey', 'San Pedro']\n",
    "df_jobs = df_jobs[df_jobs['Location'].isin(filtered_location_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_url(url):\n",
    "    parts = url.split('?position')\n",
    "    return parts[0]\n",
    "df_jobs['Url'] = df_jobs['Url'].apply(truncate_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = df_jobs.drop_duplicates(subset=['Location', 'Title', 'Company']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {\n",
    "    'Data Engineering': ['Data Engineer', 'ETL', 'Data Platform', 'Data Pipeline', \n",
    "                         'Database Engineer', 'Big Data', 'Hadoop', 'Spark', \n",
    "                         'Databricks', 'Data Integration', 'Data Warehouse',\n",
    "                         'Data Monitoring', 'Data Governance and Management'],\n",
    "    \n",
    "    'Data Analysis': ['Data Analyst', 'Business Intelligence', 'BI', 'Data Visualization', \n",
    "                      'Data Reporting', 'SQL', 'Data Metrics', 'Analytics', 'Data Insights',\n",
    "                      'Analítica de Datos', 'Data Operations Analyst'],\n",
    "    \n",
    "    'Data Science': ['Data Scientist', 'Machine Learning', 'ML', 'Statistical Analysis', \n",
    "                     'Predictive Modeling', 'Data Modeling', 'Deep Learning', \n",
    "                     'Algorithm', 'Statistical', 'Data Science Analyst'],\n",
    "    \n",
    "    'AI/ML': ['AI', 'Artificial Intelligence', 'Machine Learning', 'ML', 'Neural Networks', \n",
    "              'Deep Learning', 'AI Engineer', 'AI/ML Engineer', 'MLOps', 'Model Training',\n",
    "              'IA'],\n",
    "    \n",
    "    'Software Engineering': ['Software Engineer', 'Developer', 'Backend Developer', \n",
    "                              'Frontend Developer', 'Fullstack Developer', 'Software Development', \n",
    "                              'Programming', 'App Developer', 'Application Developer', \n",
    "                              'DevOps', 'Desarrollador']\n",
    "}\n",
    "\n",
    "def get_category(title):\n",
    "    title_lower = title.lower()\n",
    "    for category, keywords in categories.items():\n",
    "        for keyword in keywords:\n",
    "            if keyword.lower() in title_lower:\n",
    "                return category\n",
    "    return 'Other'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AI/ML' 'Data Science' 'Software Engineering' 'Data Analysis' 'Other'\n",
      " 'Data Engineering']\n"
     ]
    }
   ],
   "source": [
    "# Apply the function to the 'Job_Title' column\n",
    "df_jobs['Category'] = df_jobs['Title'].apply(get_category)\n",
    "x = df_jobs['Category'].unique()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AI/ML' 'Data Science' 'Data Analysis']\n"
     ]
    }
   ],
   "source": [
    "categories_to_drop = ['Other', 'Software Engineering','Data Engineering']\n",
    "df_jobs = df_jobs[~df_jobs['Category'].isin(categories_to_drop)].reset_index(drop=True)\n",
    "x = df_jobs['Category'].unique()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export DataFrame to CSV\n",
    "date = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "position = position.replace(\" \", \"_\")\n",
    "file_name = f'LinkedIn_{position}_{location}_{date}.csv'\n",
    "df_jobs.to_csv(file_name, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
