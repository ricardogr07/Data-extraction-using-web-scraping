{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (17.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyarrow) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ricar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries if they are not present\n",
    "!pip install requests\n",
    "!pip install beautifulsoup4\n",
    "!pip install pyarrow\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_posted_dict = {\n",
    "    'ALL': '',\n",
    "    'MONTH': 'r2592000',\n",
    "    'WEEK': 'r604800',\n",
    "    'DAY': 'r86400'\n",
    "}\n",
    "remote_dict = {\n",
    "    'ALL': '',\n",
    "    'ON-SITE': '1',\n",
    "    'REMOTE': '2',\n",
    "    'HYBRID': '3'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_user_agent():\n",
    "\n",
    "    headers = [\n",
    "        {'User-Agent': 'Mozilla/5.0'},\n",
    "        {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.84 Safari/537.36'},\n",
    "        {'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Mobile Safari/537.36'},\n",
    "        {'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Mobile Safari/537.36'},\n",
    "        {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.84 Safari/537.36'}\n",
    "    ]\n",
    "\n",
    "    selected_header = random.choice(headers)\n",
    "    return selected_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_main_linkedin_url(position, location, distance=10, time_posted='ALL', remote='ALL'):\n",
    "   \n",
    "    # Base URL for LinkedIn job search\n",
    "    base_url = 'https://www.linkedin.com/jobs/search/'\n",
    "    \n",
    "    # Replace spaces in position with URL encoding\n",
    "    url_friendly_position = position.replace(\" \", \"%20\")\n",
    "    \n",
    "    # Construct the query parameters\n",
    "    query_params = f'?keywords={url_friendly_position}&location={location}'\n",
    "    \n",
    "    if distance:\n",
    "        query_params += f'&distance={distance}'\n",
    "    if time_posted:\n",
    "        time_posted_value = time_posted_dict.get(time_posted, '')\n",
    "        query_params += f'&f_TPR={time_posted_value}'\n",
    "    if remote:\n",
    "        remote_value = remote_dict.get(remote, '')\n",
    "        query_params += f'&f_WT={remote_value}'\n",
    "    \n",
    "    # Combine base URL with query parameters\n",
    "    url_search = base_url + query_params\n",
    "    \n",
    "    return url_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = 'Data Scientist'\n",
    "location = 'Monterrey'\n",
    "time_posted = 'ALL'\n",
    "remote = 'ALL'\n",
    "\n",
    "header = get_random_user_agent()\n",
    "\n",
    "main_url = generate_main_linkedin_url(position, location,time_posted=time_posted, remote=remote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_jobs_until_success(url):\n",
    "    got_200 = False\n",
    "    while not got_200:\n",
    "        response = requests.get(url, headers=get_random_user_agent())\n",
    "        got_200 = response.status_code == 200\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 570 jobs that will be scraped based on the given conditions.\n"
     ]
    }
   ],
   "source": [
    "response = fetch_jobs_until_success(main_url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "all_jobs = int(soup.find('span', {'class': 'results-context-header__job-count'}).text)\n",
    "print(f'There are a total of {all_jobs} jobs that will be scraped based on the given conditions.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_next_10_positions(position, location,start_position, distance=10, time_posted='ALL', remote='ALL'):\n",
    "   \n",
    "    # Base URL for LinkedIn job search\n",
    "    base_url = 'https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search'\n",
    "    \n",
    "    # Replace spaces in position with URL encoding\n",
    "    url_friendly_position = position.replace(\" \", \"%20\")\n",
    "    \n",
    "    # Construct the query parameters\n",
    "    query_params = f'?keywords={url_friendly_position}&location={location}'\n",
    "    \n",
    "    if distance:\n",
    "        query_params += f'&distance={distance}'\n",
    "    if time_posted:\n",
    "        time_posted_value = time_posted_dict.get(time_posted, '')\n",
    "        query_params += f'&f_TPR={time_posted_value}'\n",
    "    if remote:\n",
    "        remote_value = remote_dict.get(remote, '')\n",
    "        query_params += f'&f_WT={remote_value}'\n",
    "    query_params += f'&position=1&pageNum=0&start={start_position}'\n",
    "    \n",
    "    # Combine base URL with query parameters\n",
    "    url_search = base_url + query_params\n",
    "    \n",
    "    return url_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing data for page: 1/57\n",
      "Parsing data for page: 2/57\n",
      "Parsing data for page: 3/57\n",
      "Parsing data for page: 4/57\n",
      "Parsing data for page: 5/57\n",
      "Parsing data for page: 6/57\n",
      "Parsing data for page: 7/57\n",
      "Parsing data for page: 8/57\n",
      "Parsing data for page: 9/57\n",
      "Parsing data for page: 10/57\n",
      "Parsing data for page: 11/57\n",
      "Parsing data for page: 12/57\n",
      "Parsing data for page: 13/57\n",
      "Parsing data for page: 14/57\n",
      "Parsing data for page: 15/57\n",
      "Parsing data for page: 16/57\n",
      "Parsing data for page: 17/57\n",
      "Parsing data for page: 18/57\n",
      "Parsing data for page: 19/57\n",
      "Parsing data for page: 20/57\n",
      "Parsing data for page: 21/57\n",
      "Parsing data for page: 22/57\n",
      "Parsing data for page: 23/57\n",
      "Parsing data for page: 24/57\n",
      "Parsing data for page: 25/57\n",
      "Parsing data for page: 26/57\n",
      "Parsing data for page: 27/57\n",
      "Parsing data for page: 28/57\n",
      "Parsing data for page: 29/57\n",
      "Parsing data for page: 30/57\n",
      "Parsing data for page: 31/57\n",
      "Parsing data for page: 32/57\n",
      "Parsing data for page: 33/57\n",
      "Parsing data for page: 34/57\n",
      "Parsing data for page: 35/57\n",
      "Parsing data for page: 36/57\n",
      "Parsing data for page: 37/57\n",
      "Parsing data for page: 38/57\n",
      "Parsing data for page: 39/57\n",
      "Parsing data for page: 40/57\n",
      "Parsing data for page: 41/57\n",
      "Parsing data for page: 42/57\n",
      "Parsing data for page: 43/57\n",
      "Parsing data for page: 44/57\n",
      "Parsing data for page: 45/57\n",
      "Parsing data for page: 46/57\n",
      "Parsing data for page: 47/57\n",
      "Parsing data for page: 48/57\n",
      "Parsing data for page: 49/57\n",
      "Parsing data for page: 50/57\n",
      "Parsing data for page: 51/57\n",
      "Parsing data for page: 52/57\n",
      "Parsing data for page: 53/57\n",
      "Parsing data for page: 54/57\n",
      "Parsing data for page: 55/57\n",
      "Parsing data for page: 56/57\n",
      "Parsing data for page: 57/57\n"
     ]
    }
   ],
   "source": [
    "jobs = []\n",
    "total_pages = math.ceil(all_jobs/10)\n",
    "for i in range(0,all_jobs, 10):\n",
    "    \n",
    "    current_page = i/10+1\n",
    "\n",
    "    target_url = get_url_next_10_positions(position, location,i,time_posted=time_posted,remote=remote)\n",
    "\n",
    "    response = fetch_jobs_until_success(target_url)\n",
    "    \n",
    "    print(f\"Parsing data for page: {int(current_page)}/{total_pages}\")\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    alljobs = soup.find_all('li')\n",
    "\n",
    "    for job in alljobs:\n",
    "        try:\n",
    "            info = job.find('div', class_=\"base-search-card__info\")\n",
    "            title = info.find('h3', class_=\"base-search-card__title\").text.strip() if info else 'N/A'\n",
    "            company = info.find('h4', class_=\"base-search-card__subtitle\").text.strip() if info else 'N/A'\n",
    "\n",
    "            metadata = job.find('div', class_=\"base-search-card__metadata\")\n",
    "            location_element = metadata.find('span', class_=\"job-search-card__location\") if metadata else None\n",
    "            location_job = location_element.text.strip() if location_element else 'N/A'\n",
    "\n",
    "            joburl_element = job.find('a', class_=\"base-card__full-link\")\n",
    "            joburl = joburl_element['href'] if joburl_element else 'N/A'\n",
    "\n",
    "            job_info = {\n",
    "                'Location': location_job,\n",
    "                'Title': title,\n",
    "                'Company': company,\n",
    "                'Url': joburl\n",
    "            }\n",
    "\n",
    "            jobs.append(job_info)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing job: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = pd.DataFrame(jobs, columns=['Location', 'Title', 'Company', 'Url'])\n",
    "df_jobs.replace(\"N/A\", pd.NA, inplace=True)\n",
    "df_jobs = df_jobs.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export DataFrame to CSV\n",
    "date = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "position = position.replace(\" \", \"_\")\n",
    "# Start with the base file name\n",
    "file_name = f'LinkedIn_Jobs_{position}_{location}'\n",
    "\n",
    "if time_posted != 'ALL':\n",
    "    file_name += f'_LAST_{time_posted}'\n",
    "\n",
    "# Append remote if it's not 'ALL'\n",
    "if remote != 'ALL':\n",
    "    file_name += f'_{remote}'\n",
    "\n",
    "# Append the date to the file name\n",
    "file_name += f'_{date}.csv'\n",
    "\n",
    "# Export DataFrame to CSV\n",
    "df_jobs.to_csv(file_name, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
